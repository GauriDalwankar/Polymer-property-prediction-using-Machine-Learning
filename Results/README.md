# RESULTS
Hyperparameter tuning improves the overall performance of a machine learning model by finding a set of optimal hyperparameter values which maximizes the modelâ€™s performance, minimizes loss and produces better results. Before tuning, RMSE of random forest for crystallization tendency prediction was 18.03, 19.6, 17.89 for RDKIT fingerprint, our algorithm, and Mordred fingerprint respectively. For tuning random forest model predictions, the parameters chosen are n_estimators and max_depth. N_estimators is the number of trees in the forest. With an increase in the number of trees, the precision of the outcome increases but the model becomes slower. Therefore choosing a n_estimator value that your processor can handle allows your model to be more stable and perform well. While n_estimator has a tradeoff between speed & score, max_depth has the possibility of improving both.  By limiting the depth of your trees, one can reduce overfitting. After tuning with these two parameters for random forest, RMSE decreased to 17.62, 18.2, and 16.73 respectively. A similar procedure is repeated for the bandgap property which is a bigger dataset with a small range giving better results. 
